1) P0: Beta regression model - experiment with different strategies for creating the training data. Then get the p-scores for each action and see which strategy makes good bets rise to the top (in terms of p-scores). #prob 
2) P1: Find the derivative of the function, $cos(x)$ in two ways: #math/algebra 
	1) Write it in Taylor series and take the derivative.
	2) Using first principal, with limits.
3) P0: Prove that $y \log(x) = log(x^y)$. Write it out in Obsidian.
4) P0: Write the Taylor series expansion of: $x^{1/n}$ and also of $\log(x)$. Then use them to prove: $x^{1/n} = 1+log(x)/n + O(1/n^2)$ #math/algebra 
5) P0: Complete section on Chapman Kolmogrov equations (Ross book). #prob 
6) P0: Using induction, prove that in a markov chain with transition matrix, $P$, given that we start in state $i$, the probability we'll be in state $j$ after $n$ steps is $P^n_{i,j}$
7) P1: Get the actual coins in the min coins needed for change problem in O(n) time. #cs/dynprogr 
	1) Hint: Maintain an array similar to dp array (same size) where you store the largest coin used to make the amount. See code below.
8) P2: You generate 10 uniform random numbers. Then take the difference of the seventh largest and third largest. What is the distribution of this number? Use gradient descent to get the parameters.
9) P2: Write efficient code for longest increasing sub-sequence where you actually get the subsequence, not just its length.

Solutions here.


# Ans 1 
It reaches about the same performance as simple logistic regression - 55% accuracy. I think that's a good sign. Can we try adding nonlinearity to it? Currently my neural network achieves 60% accuracy. Next I will also do the propagating bets up and compare the top recommended bets with logistic regression vs beta model and see how similar/different they are.


# Ans 2
a. ![[taylor_series_cos.jpg]]

b. ![[cos_first_principles_incomplete.jpg]] stuck with the algebraic simplications. Tried googling it and there's a lot of weird manipulations that I'd be lying to myself if I claimed to know.

# Ans 3 ![[log_exponent_proof.jpg]]

# Ans 4 ![[taylor_series_proof_incomplete.jpg]]

# Ans 5
done

# Ans 6
tried my best, but I have a feeling this is incomplete because 1. I only showed it for a 2x2 matrix and 2. how do we know it works for n -> infinity transitions
![[transition_matrix_induction_incomplete.jpg]]
# Ans 7
Here is the code for getting the min number of coins as a reminder.
![[MinCoinsChange#Code]]

~~~ Python
def coinChange(coins, amount):        
    dp=[math.inf] * (amount+1)
    coins_stored = [math.inf] * (amount+1)
    dp[0]=0

    for coin in coins:
        for i in range(coin, amount+1):
            if dp[i-coin]+1 < dp[i]:
                dp[i] = dp[i-coin]+1
                coins_stored[i] = coin
    coins_used = []
    remaining = amount
    while remaining>0:
        coin = coins_stored[remaining]
        coins_used.append(coin)
        remaining-=coin

    return dp[-1],coins_used
~~~


# Ans 8 Beta(3,7)

~~~Python
from scipy.stats import uniform, beta
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

dists = [uniform.rvs(size=1000) for val in range(10)]
comparison_df = pd.DataFrame(dists).T

def third_ranked_value(row):
  return sorted(row)[-3]
def seventh_ranked_value(row):
  return sorted(row)[-7]



comparison_df['3rd_ranked'] = comparison_df.apply(third_ranked_value,axis=1)
comparison_df['7th_ranked'] = comparison_df.apply(seventh_ranked_value,axis=1)
comparison_df['diff'] = comparison_df['3rd_ranked'] - comparison_df['7th_ranked']

def log_likelihood_beta(a,b,data):
  log_likelihood = 0
  for point in data:
    pdf_beta =  beta.pdf(point, a, b)
    log_likelihood+=np.log(pdf_beta)
  return log_likelihood

def compute_gradient(a,b,which,data):
  if which=='a':
    return log_likelihood_beta(a+.001,b,data) - log_likelihood_beta(a,b,data)
  else:
    return log_likelihood_beta(a,b+.001,data) - log_likelihood_beta(a,b,data)

data = comparison_df['diff'].values

a_global = 2
b_global = 1
min_likelihood = np.inf*-1
delta = np.inf
previous_delta = None

while delta > .1:
  log_likelihood = log_likelihood_beta(a_global,b_global,data)
  if min_likelihood == np.inf*-1:
    min_likelihood = log_likelihood
    delta = abs(log_likelihood)
  else:
    delta = abs(log_likelihood)
    if abs(delta-previous_delta)<.1:
      break

  alpha_grad = compute_gradient(a_global,b_global,'a',data)
  beta_grad = compute_gradient(a_global,b_global,'b',data)
  a_global+=alpha_grad
  b_global+=beta_grad
  print(a_global,b_global)
  print()
  previous_delta = delta

