1) P0: Code that creates binary search tree from root matrix using this: https://github.com/clemtoy/pptree
2) P0: Prove for the case of one independent variable that linear regression coefficients are the same as correlation coefficient.
3) P1: Summarize your understanding of what AUROC is: https://towardsdatascience.com/interpreting-auroc-in-hypothesis-testing-a45f6f757a62.
4) P1: Generate all permutations of an array. (https://www.baeldung.com/cs/array-generate-all-permutations)
5) P1: Design a database that stores all information needed for a referral system for AI betting edge. 
	1) What are the tables in the database? What does each of them contain?
	2) The columns of each table.
	3) When should records be updated, deleted, inserted in each of the tables?
6) P2: Create a simple HTML page with a single button. Feel free to look up samples online. Run the website on your local host.

1 
~~~python
import numpy as np
from ppbtree import *

keys = [10, 15, 20,25,30]
p = [.2, .2,.2,.2,.2]

w = np.zeros((len(p),len(p)))
e = np.zeros((len(p),len(p)))
root = np.zeros((len(p),len(p)))

for buffer in range(len(w)):
	for val in range(len(w)):
		if val+buffer < len(w):
			row = val
			col = val+buffer 
			w[row][col] = w[row][col-1] + p[col]
			e[row][col] = float('inf')
			for r in range(row,col+1):
				print(r)
				pr = p[r]
				left_e,left_w,right_e,right_w = None,None,None,None
				if r-1 < row:
					left_e,left_w = 0,0
				else:
					left_e = e[row,r-1]
					left_w = w[row,r-1]
				if r+1 > col:
					right_e,right_w = 0,0
				else:
					right_e = e[r+1,col]
					right_w = w[r+1,col]
				t = pr + left_e + left_w + right_e + right_w
				if t < e[row][col]:
					e[row][col] = t
					root[row][col] = r



global tree 
tree = Node()


def construct_optimal_bst(root,i,j,last):
	i,j = int(i),int(j)
	if i > j:
		return
	if last == -1:
		print(root[i][j],' is the root')
		add(tree,keys[int(root[i][j])])
	elif j < last:
		print(root[i][j],' is the left subtree')
		add(tree,keys[int(root[i][j])])
	else:
		print(root[i][j], 'is the right subtree')
		add(tree,keys[int(root[i][j])])

	construct_optimal_bst(root,i,root[i][j]-1,root[i][j])
	construct_optimal_bst(root,root[i][j]+1,j,root[i][j])
construct_optimal_bst(root,0,4,-1)

print_tree(tree, nameattr='value')
~~~
2![[correlation_regression.jpg]]

3 We have a radar that's meant to detect enemy aircrafts in the sky by detecting radiowaves. The goal is obviously that the radar detects all enemy airplanes, while not bothering the military response team when it's just a bird or any other signal that's not an enemy plane. A decision threshold is set based on the strength of the radiowave, and if the number is greater than the threshold, the military sends in response aircrafts, while if the number is less than the threshold, the military does nothing. There are 4 possible things that can happen. 

1. True positive. The radar gives a number that is above the decision threshold, a military response is triggered, and there was actually an enemy plane in the sky
2. False positive. The radar gives a number that is above the decision threshold, a military response is triggered, and there was no enemy plane in the sky
3. True negative. The radar gives a number that is below the decision threshold, a military response is not triggered, and there was no enemy plane in the sky
4. False negative. The radar gives a number that is below the decision threshold, a military response is not triggered, and there was an enemy plane in the sky.
The green outline distribution represents all the radar readings conditioned on the fact that there was no enemy aircraft in the sky. The red outline distribution represents all the radar readings conditioned on the fact that there were radar readings in the sky. The yellow line represents the decision boundary. In an ideal world, we'd have two distributions with 0 overlap, separated by a yellow line. But, in reality, there is some overlap between the two distributions, and that's where we face tradeoffs in our choice of decision threshold.

Of the data points where the ground truth value is that there's an enemy airplane in the sky, the ones that fall below the decision threshold are the False Negatives, and the False Negative rate is that False Negative number divided by False Negative + True Positive. For all the rates, the denominator is the number of ground truths in the correct direction of the numerator (if numerator is False Positive, the correct direction is True Negative). For example the True Positive rate is the True Positives divided by True Positives + False Negatives. 

The higher ups in command want us to decrease the decision threshold because they don't want us missing any enemy attacks. They want us to lower the false negative rate and aren't that concerned with the subsequent increase in the false positive rate. The military response team wants to increase the decision threshold so as to decrease the false positives, because they don't want to keep loading up their airplanes with missilles and taking off only to find a big bird in the sky. 

The true positive rate is also called the recall.

Precision is not one of the 4 rates, but rather True Positive divided by True Positive+False positive. 

#stats/hypothtst/auroc keywords: auroc hypothesis testing ml false positive false negative precision recall


4
~~~python
global sequence,counter
sequence = [1,2,3]
counter = 0


def gen_permutations(current):
	global sequence,counter
	if len(current)==len(sequence):
		print(current)
		counter+=1
		return
	for val in sequence:
		if val not in current: 
			current_copy = current.copy()
			current_copy.append(val)

			gen_permutations(current_copy)


gen_permutations([])
~~~




5
[![[AIBE_schema.csv]]
]


6
~~~html 
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Simple Button Page</title>
</head>
<body>

  <button onclick="handleButtonClick()">Click me!</button>

  <script>
    function handleButtonClick() {
      alert("Button clicked!");
    }
  </script>

</body>
</html>
~~~




