- P0: Complete complexity (avg case) of simplex method.
- P0: Do exercise 6 of chapter-4, Ross.
- Transformers, 3B1B videos: https://www.youtube.com/watch?v=XSSTuhyAmnI, https://www.youtube.com/watch?v=wjZofJX0v4M. And attention is all you need paper: https://arxiv.org/pdf/1706.03762Compile all questions in Obsidian.
- P1: Read first chapter of geometric linear algebra book.
- P1: Read boolean satisfiability section from Ross.



Avg case of simplex is pushed on previous weeks assignment already.

Transformers notes:
Attention mechanism steps: **Review Rohit's Linear Algebra stuff on Matrix Multiplication. Understand it inside out**
1. multiply each E (embedding) vector by a Wq matrix to get a Q vector for each embedding (word). Q vector can be interpreted as a query that's asking "are there any adjectives that came before me" - how does it do this is completely out of the scope right now, just get the mechanics first.
2. multiply each E (embedding) vector by a Wk matrix which is the key matrix and it gives a K vector for each embedding. K vectors can be interpreted as a response to the query saying "Yes im an adjective, I'm here". how does it do this is completely out of the scope right now, just get the mechanics first.
3. It seems that K vectors and Q vectors live in the same space which is much lower dimension than the embedding space.
4. Now make the first manmade matrix where the row indices represent Key vectors for an embedding and the column indices represent the query vectors for an embedding. You take the dot product between every key vector and query vector and the entries of these matrix are the embedding. When the vectors are very similar, the dot product will be big. Before the next step, they divide each entry of the matrix by the square root of the dimension of the vector. Why????
5. We want each column to be between 0 and 1 and add up to 1. Not negative infinity to infinity. For this, we use softmax. Before softmax set the lower diagonal to negative infinity because we only want to consider pairs where the index of the query is greater than or equal to the index of the key **REVIEW SOFTMAX, UNDERSTAND IT LIKE BACK OF HAND**
6. The above key query stuff just identifies which words are relevant to which other words. Next we need to actually update the values of the embedding vector based on the relevant adjectives identified. To do this we have a third matrix called the Value matrix. This matrix is multiplied by the adjectives (only adjectives I think?) and the idea behind it's parameters are that we want to learn values such that when the adjective embedding vectors are multiplied by this matrix, is leads to a value vector which adds the appropriate values to the noun vectors so that it's new position in the embedding space is contextually relevant. But before we add we incorporate weights in the next step so we add the right amount.
7. Next there's a second manmade matrix where the row indices are the value vectors for each embedding, the column vectors are the embedding indices, and the entries of each cell of the matrix are the value vectors multiplied by the key-query probabilities from step 4 and 5. It's essentially leading to weighted value vectors where the weights are how much each adjective matches up with each noun.
