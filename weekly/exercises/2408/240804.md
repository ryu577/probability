- P0: Worst case time complexity for binary search and exact comparisons.
- P1: Average case time complexity for binary search and exact comparisons.
- P0: Watch 3B1B transformers video and compile questions: https://www.youtube.com/watch?v=XSSTuhyAmnI.
- Watch history video: 
- P1: Read boolean satisfiability example 4.29 from Ross.
- P2: Exercise 13, chapter 4 from Ross.



P0 Worst case time complexity binary search #bst #binarysearch #cs #algorithms 
![[worst_case_binary_search_1-min.jpg]]

![[worst_case_binary_search_2-min.jpg]]

Average case complexity binary search:
#binarysearch #bst #algorithms #cs 
![[aveage_case_binary_search_pt1-min.jpg]]
![[average_case_binary_search_pt2-min.jpg]]

P0: Transformers 3B1B video #transformers #llm #attention
Transformers notes:
Attention mechanism steps: **Review Rohit's Linear Algebra stuff on Matrix Multiplication. Understand it inside out**
1. multiply each E (embedding) vector by a Wq matrix to get a Q vector for each embedding (word). Q vector can be interpreted as a query that's asking "are there any adjectives that came before me" - how does it do this is completely out of the scope right now, just get the mechanics first.
2. multiply each E (embedding) vector by a Wk matrix which is the key matrix and it gives a K vector for each embedding. K vectors can be interpreted as a response to the query saying "Yes im an adjective, I'm here". how does it do this is completely out of the scope right now, just get the mechanics first.
3. It seems that K vectors and Q vectors live in the same space which is much lower dimension than the embedding space.
4. Now make the first manmade matrix where the row indices represent Key vectors for an embedding and the column indices represent the query vectors for an embedding. You take the dot product between every key vector and query vector and the entries of these matrix are the embedding. When the vectors are very similar, the dot product will be big. Before the next step, they divide each entry of the matrix by the square root of the dimension of the vector. Why????
5. We want each column to be between 0 and 1 and add up to 1. Not negative infinity to infinity. For this, we use softmax. Before softmax set the lower diagonal to negative infinity because we only want to consider pairs where the index of the query is greater than or equal to the index of the key **REVIEW SOFTMAX, UNDERSTAND IT LIKE BACK OF HAND**
6. The above key query stuff just identifies which words are relevant to which other words. Next we need to actually update the values of the embedding vector based on the relevant adjectives identified. To do this we have a third matrix called the Value matrix. This matrix is multiplied by the adjectives (only adjectives I think?) and the idea behind it's parameters are that we want to learn values such that when the adjective embedding vectors are multiplied by this matrix, is leads to a value vector which adds the appropriate values to the noun vectors so that it's new position in the embedding space is contextually relevant. But before we add we incorporate weights in the next step so we add the right amount.
7. Next there's a second manmade matrix where the row indices are the value vectors for each embedding, the column vectors are the embedding indices, and the entries of each cell of the matrix are the value vectors multiplied by the key-query probabilities from step 4 and 5. It's essentially leading to weighted value vectors where the weights are how much each adjective matches up with each noun.




P1: Boolean Satisfiability. 

The big idea here is that he's used probability to come up with a heuristic for when you can stop searching for a solution to an NP hard problem. As we know, NP hard problems cannot be solved in polynomial time, so you have have to keep going and going. It would be nice to know if there's a stopping point where one can be almost certain there's no feasible solution.

Let's first describe the problem. We have a set of variables X1, X2.. Xn, and a set of clauses, C1, C2 .. Cn. Each clause is made up of two variables. A clause is satisfied is any of the 2 variables are true. And the entire formula (comprised of multiple clauses) is true only if ALL the clauses are true. So for example if X1 = True and X2 = False and X3 = True then a formula with (X1+X2)*(X2+X3) would be True since both clauses are true.

Next, let's describe his proposed algorithm for finding the solution to the problem described above. The first step is to randomly select any one of the clauses which are FALSE. The second step is to randomly flip the sign of ONE of the variables in the clause. His claim is that if you have not stopped after n^2 * some constant term, then there's a probability close to 1 that there is no solution.

Next he introduces a variable called Yj, which represents at every iteration the number of clauses that are set to true. The key thing to note is that Yj grows by 1 at each step AT LEAST 50% of the time (more specifically exactly 50% or 100% depending on how many of the variables in the clause are wrong), and Yj decreases by 1 AT MOST 50% of the time (more specifically 0% or 50% of the time). 

Another key thing to note is that Yj itself isn't a markov chain because the state does not contain all the information necessary to know how many clauses are true or false at any given state. Put differently, if we knew all of the Yj values from the beginning till the end, it would be much easier to figure out what Yj+1 would be. And this violates the Markov Property which is that the probability of Yj+1 is only dependent on Yj and independent of any states prior to Yj.

Now he showed in the section above that when p is equal to 1/2, the complexity of going from 0 to N in the markov chain example is N^2. So we know that this is AT WORST N^2 here. The thing I'm a bit confused about is how are we connecting the markov chain from the previous example to this boolean satisfiability problem?



Exercise 13: #ross #probability #exercises
Let P be the transition probability matrix of a Markov chain. Argue that if for some
positive integer r, Pr has all positive entries, then so does Pn, for all integers n  r.

The whole idea here is to show that a markov chain transition matrix with some 0 entries that eventually in a later time step becomes a matrix with no 0 entries can never go back to a matrix with some zero entries. Here I will explain why:

The only way a matrix P^k can have a 0 entry in it would be if the dot product of some row in P^k-1 and some column in P is equal to zero. We know that no row in P^k-1 can be all zeros because the rows of a transition matrix must all sum to 1. And we know that there can't be a column of P with all zeros, because if there were, it would never be able to reach a state P^r with all non zero entries. A column of all zeros is essentially saying that you cannot get to the state represented by that column index from any state - you can't even get to it from itself. So the matrix would never become fully non zero in the first place. Paradoxical. 






