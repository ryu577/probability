1) P0: Write the Taylor series expansion of: $x^{1/n}$ and also of $\log(x)$. Then use them to prove: $x^{1/n} = 1+log(x)/n + O(1/n^2)$ #math/algebra 
2) P0: Using induction, prove that in a markov chain with transition matrix, $P$, given that we start in state $i$, the probability we'll be in state $j$ after $n$ steps is $P^n_{i,j}$
3) P1: Write efficient code for longest increasing sub-sequence where you actually get the subsequence, not just its length.
4) P2: Exercise 15-12 of "Introduction to Algorithms" by Cormen. Come up with an approach for solving it.
5) P2: Trignometry: prove that $1+\sec^2(x)=\tan^2(x)$
6) P0: On page 203 of the Ross book, you have some expression for $\alpha$ which is being derived step by step. At some point, you get a summation with $P_{r,j}$. But in the previous step, that term has $X_0=i$ in the conditioning. Explain why $i$ just vanished from the expression.
	1) Then read upto proposition 4.1.
7) P0: Logistic regression for work project. Implement the logistic regression pricing model below:

Logistic regression
$$p(x) = \frac{1}{1+e^{-\beta^T x}}$$
Here, $x$ is the vector of features.
It consists of the price you set, $m$ along with other features like prime time slot, nature of show, etc. We treat $m$ separately from the other features, $f_1, f_2, \dots f_n$. So the vector $x$ looks like $[m, 1, f_1, f_2, \dots f_n]$ (the $1$ is for the constant term). The coefficient vector will look like: $\beta: [\alpha, \beta_0,\beta_1,\dots \beta_n]$.

Further, we can call the feature vector $f: [f_1, f_2 \dots f_n]$ and the corresponding slice of the coefficient vector $\beta_f: [\beta_1, \beta_2, \dots \beta_n]$.

Once the coefficients of the $f$ vector are set, that term becomes a constant since it doesn't depend on $m$. We get the probability that the ad will be brought by someone:

$$p(m) = \frac{1}{1+e^{-\beta_0}e^{-\beta_f^T f} e^{-\alpha m}}$$
The expected payoff (setting everything that doesn't depend on $m$ as a constant, $c$)
$$E_M = m.p(m) = \frac{m}{1+c e^{-\alpha m}}$$
To maximize this, we take the derivative with respect to $m$ and set it to $0$. Working through the algebra we get:
$$(1+c e^{-\alpha m}) = -(c\alpha)m e^{-\alpha m}$$
Simplifying:
$$e^{\alpha m} = -c- (c \alpha)m$$
The left side is an exponential in $m$ and the right side is a linear function in $m$. Note that $\alpha$ will be negative since the probability of the slot selling should go down with $m$ (the price demanded). The two equations will meet at $m_0$, which is our point of interest.

![[Screenshot 2023-09-17 at 4.15.35 PM.png#invert]]

How do we find $m_0$? https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fsolve.html
Again, the $c$ and $\alpha$ come from the logistic regression.